{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651eb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6c817",
   "metadata": {},
   "source": [
    "Only run this code if you have a GPU. This part of the code makes the code run on your GPU, I used the tensorflow-gpu version 1.15 for this to work. It's considerably faster using my GPU (NVIDIA GTX 1060 6GB), than my CPU. About ~5 times faster, depending on the dataset I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8de405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:27:00.0, compute capability: 6.1\n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9731192535778866208\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5083824128\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1724886082556085304\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:27:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48863882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9037f96e",
   "metadata": {},
   "source": [
    "Several different datasets I experimented on, they are listed from short to long. The shortest one takes a couple of seconds to train, the longest one can, depending on your computer, take hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aed777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29088\n"
     ]
    }
   ],
   "source": [
    "with open('wikiped.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff03120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128866\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"jokes.csv\")\n",
    "data = ' '.join(df['Joke'].tolist()).replace(\"\\'\", \"\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "616a6cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797205\n"
     ]
    }
   ],
   "source": [
    "url = \"http://gutenberg.org/files/1342/1342-0.txt\"\n",
    "book = requests.get(url)\n",
    "data = book.text\n",
    "data = data[2440:]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be15dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396753\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/24869/24869-0.txt\"\n",
    "book = requests.get(url)\n",
    "data = book.text\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c1147",
   "metadata": {},
   "source": [
    "Cleaning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5627c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset, char_filter = r\"[^\\w]\"):\n",
    "\n",
    "    # convert words to lower case\n",
    "    dataset = dataset.lower()\n",
    "    dataset = dataset.replace(\"창\", \"a\")\n",
    "    # tokenise words\n",
    "    words = word_tokenize(dataset)\n",
    "\n",
    "    # strip whitespace from all words\n",
    "    words = [word.strip() for word in words]\n",
    "\n",
    "        \n",
    "    # join back words to get dataset\n",
    "    dataset = \" \".join(words)\n",
    "\n",
    "    # remove unwanted characters\n",
    "    dataset = re.sub(char_filter, \" \", dataset)\n",
    "\n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    dataset = re.sub(r\"\\s+\", \" \", dataset)\n",
    "\n",
    "    # strip whitespace from dataset\n",
    "    dataset = dataset.strip()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data = clean_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2b958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife however little known the feelings or views of such a man may be on his first entering a neighbourhood this truth is so well fixed in the minds of the surrounding families that he is considered the rightful property of some one or other of their daughters a my dear mr bennet a said his lady to him one day a have you heard that netherfield park is let at last a mr bennet replied that he had not a but it is a returned she a for mrs long has just been here and she told me all about it a mr bennet made no answer a do you not want to know who has taken it a cried his wife impatiently a _you_ want to tell me and i have no objection to hearing it a this was invitation enough a why my dear you must know mrs long says that netherfield is taken by a young man of large fortune from the north of england that he came down on monday in a chaise and four to see the place and was so much d\n"
     ]
    }
   ],
   "source": [
    "print(data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ab50d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokeniser = Tokenizer()\n",
    "word_tokeniser.fit_on_texts([data])\n",
    "encoded_words = word_tokeniser.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca6e401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 6877\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f5bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "MAX_SEQ_LENGTH = 10\n",
    "\n",
    "for i in range(MAX_SEQ_LENGTH, len(encoded_words)):\n",
    "    sequence = encoded_words[i-MAX_SEQ_LENGTH:i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d30d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the sequence into X and y\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = sequences[:80000,:-1]  # assign all but last words of a sequence to X\n",
    "y = sequences[:80000,-1]   # assign last word of each sequence to y\n",
    "y = to_categorical(y, num_classes=VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f92cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f93df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mx\\anaconda3\\envs\\a\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 100)           687700    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           117248    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6877)              887133    \n",
      "=================================================================\n",
      "Total params: 1,823,665\n",
      "Trainable params: 1,823,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model architecture\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH))\n",
    "\n",
    "# lstm layer 1\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# lstm layer 2\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(VOCABULARY_SIZE, activation='softmax'))\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "# summarize defined model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24884138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80000/80000 [==============================] - 9s 117us/step - loss: 1.4752 - accuracy: 0.6821\n",
      "Epoch 2/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 1.4519 - accuracy: 0.6864\n",
      "Epoch 3/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 1.4281 - accuracy: 0.6921\n",
      "Epoch 4/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 1.3975 - accuracy: 0.7000\n",
      "Epoch 5/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 1.3746 - accuracy: 0.7054\n",
      "Epoch 6/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 1.3468 - accuracy: 0.7115\n",
      "Epoch 7/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 1.3213 - accuracy: 0.7172\n",
      "Epoch 8/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 1.2964 - accuracy: 0.7231\n",
      "Epoch 9/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 1.2751 - accuracy: 0.7278\n",
      "Epoch 10/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 1.2535 - accuracy: 0.7319\n",
      "Epoch 11/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 1.2287 - accuracy: 0.7383\n",
      "Epoch 12/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 1.2079 - accuracy: 0.7426\n",
      "Epoch 13/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 1.1833 - accuracy: 0.7492\n",
      "Epoch 14/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 1.1616 - accuracy: 0.7542\n",
      "Epoch 15/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 1.1359 - accuracy: 0.7603\n",
      "Epoch 16/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 1.1208 - accuracy: 0.7634\n",
      "Epoch 17/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 1.0979 - accuracy: 0.7693\n",
      "Epoch 18/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 1.0809 - accuracy: 0.7710\n",
      "Epoch 19/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 1.0562 - accuracy: 0.7789\n",
      "Epoch 20/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 1.0305 - accuracy: 0.7854\n",
      "Epoch 21/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 1.0111 - accuracy: 0.7890\n",
      "Epoch 22/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.9950 - accuracy: 0.7925\n",
      "Epoch 23/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.9797 - accuracy: 0.7979\n",
      "Epoch 24/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.9643 - accuracy: 0.7998\n",
      "Epoch 25/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.9405 - accuracy: 0.8059\n",
      "Epoch 26/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.9223 - accuracy: 0.8104\n",
      "Epoch 27/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.9060 - accuracy: 0.8134\n",
      "Epoch 28/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.8839 - accuracy: 0.8185\n",
      "Epoch 29/100\n",
      "80000/80000 [==============================] - 10s 120us/step - loss: 0.8647 - accuracy: 0.8241\n",
      "Epoch 30/100\n",
      "80000/80000 [==============================] - 10s 121us/step - loss: 0.8539 - accuracy: 0.8265\n",
      "Epoch 31/100\n",
      "80000/80000 [==============================] - 9s 118us/step - loss: 0.8381 - accuracy: 0.8295\n",
      "Epoch 32/100\n",
      "80000/80000 [==============================] - 10s 121us/step - loss: 0.8286 - accuracy: 0.8316\n",
      "Epoch 33/100\n",
      "80000/80000 [==============================] - 9s 117us/step - loss: 0.8139 - accuracy: 0.8347\n",
      "Epoch 34/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.7978 - accuracy: 0.8377\n",
      "Epoch 35/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.7729 - accuracy: 0.8455\n",
      "Epoch 36/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.7490 - accuracy: 0.8513\n",
      "Epoch 37/100\n",
      "80000/80000 [==============================] - 9s 116us/step - loss: 0.7441 - accuracy: 0.8525\n",
      "Epoch 38/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.7356 - accuracy: 0.8528\n",
      "Epoch 39/100\n",
      "80000/80000 [==============================] - 9s 118us/step - loss: 0.7149 - accuracy: 0.8592\n",
      "Epoch 40/100\n",
      "80000/80000 [==============================] - 10s 123us/step - loss: 0.7062 - accuracy: 0.8609\n",
      "Epoch 41/100\n",
      "80000/80000 [==============================] - 10s 126us/step - loss: 0.7020 - accuracy: 0.8609\n",
      "Epoch 42/100\n",
      "80000/80000 [==============================] - 10s 119us/step - loss: 0.6856 - accuracy: 0.8655\n",
      "Epoch 43/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.6654 - accuracy: 0.8696\n",
      "Epoch 44/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.6501 - accuracy: 0.8736\n",
      "Epoch 45/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.6367 - accuracy: 0.8762\n",
      "Epoch 46/100\n",
      "80000/80000 [==============================] - 9s 112us/step - loss: 0.6233 - accuracy: 0.8809\n",
      "Epoch 47/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.6133 - accuracy: 0.8830\n",
      "Epoch 48/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.6062 - accuracy: 0.8835\n",
      "Epoch 49/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.6145 - accuracy: 0.8801\n",
      "Epoch 50/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.5930 - accuracy: 0.8864\n",
      "Epoch 51/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.5754 - accuracy: 0.8911\n",
      "Epoch 52/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.5561 - accuracy: 0.8959\n",
      "Epoch 53/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.5378 - accuracy: 0.9003\n",
      "Epoch 54/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.5392 - accuracy: 0.8999\n",
      "Epoch 55/100\n",
      "80000/80000 [==============================] - 9s 112us/step - loss: 0.5386 - accuracy: 0.8990\n",
      "Epoch 56/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.5217 - accuracy: 0.9027\n",
      "Epoch 57/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.5062 - accuracy: 0.9069\n",
      "Epoch 58/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.5170 - accuracy: 0.9035\n",
      "Epoch 59/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.5126 - accuracy: 0.9042\n",
      "Epoch 60/100\n",
      "80000/80000 [==============================] - 9s 112us/step - loss: 0.4981 - accuracy: 0.9068\n",
      "Epoch 61/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.4810 - accuracy: 0.9119\n",
      "Epoch 62/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.4649 - accuracy: 0.9159\n",
      "Epoch 63/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.4419 - accuracy: 0.9223\n",
      "Epoch 64/100\n",
      "80000/80000 [==============================] - 9s 112us/step - loss: 0.4449 - accuracy: 0.9206\n",
      "Epoch 65/100\n",
      "80000/80000 [==============================] - 9s 109us/step - loss: 0.4570 - accuracy: 0.9172\n",
      "Epoch 66/100\n",
      "80000/80000 [==============================] - 9s 109us/step - loss: 0.4481 - accuracy: 0.9188\n",
      "Epoch 67/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.4264 - accuracy: 0.9254\n",
      "Epoch 68/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.4167 - accuracy: 0.9267\n",
      "Epoch 69/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.4137 - accuracy: 0.9264\n",
      "Epoch 70/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.4083 - accuracy: 0.9283\n",
      "Epoch 71/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.3988 - accuracy: 0.9302\n",
      "Epoch 72/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.3966 - accuracy: 0.9305\n",
      "Epoch 73/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.3988 - accuracy: 0.9299\n",
      "Epoch 74/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.3960 - accuracy: 0.9296\n",
      "Epoch 75/100\n",
      "80000/80000 [==============================] - 9s 113us/step - loss: 0.3855 - accuracy: 0.9315\n",
      "Epoch 76/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.3647 - accuracy: 0.9368\n",
      "Epoch 77/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.3455 - accuracy: 0.9430\n",
      "Epoch 78/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.3442 - accuracy: 0.9435\n",
      "Epoch 79/100\n",
      "80000/80000 [==============================] - 9s 117us/step - loss: 0.3466 - accuracy: 0.9409\n",
      "Epoch 80/100\n",
      "80000/80000 [==============================] - 9s 118us/step - loss: 0.3460 - accuracy: 0.9413\n",
      "Epoch 81/100\n",
      "80000/80000 [==============================] - 9s 116us/step - loss: 0.3436 - accuracy: 0.9413\n",
      "Epoch 82/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.3529 - accuracy: 0.9385\n",
      "Epoch 83/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.3530 - accuracy: 0.9378\n",
      "Epoch 84/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.3366 - accuracy: 0.9431\n",
      "Epoch 85/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.3202 - accuracy: 0.9471\n",
      "Epoch 86/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.3084 - accuracy: 0.9491\n",
      "Epoch 87/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.2976 - accuracy: 0.9535\n",
      "Epoch 88/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.2942 - accuracy: 0.9532\n",
      "Epoch 89/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.2940 - accuracy: 0.9528\n",
      "Epoch 90/100\n",
      "80000/80000 [==============================] - 9s 117us/step - loss: 0.2918 - accuracy: 0.9532\n",
      "Epoch 91/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.3005 - accuracy: 0.9496\n",
      "Epoch 92/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.3032 - accuracy: 0.9486\n",
      "Epoch 93/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.3081 - accuracy: 0.9475\n",
      "Epoch 94/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.2967 - accuracy: 0.9504\n",
      "Epoch 95/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.2771 - accuracy: 0.9560\n",
      "Epoch 96/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.2557 - accuracy: 0.9618\n",
      "Epoch 97/100\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.2357 - accuracy: 0.9665\n",
      "Epoch 98/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.2450 - accuracy: 0.9635\n",
      "Epoch 99/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.2674 - accuracy: 0.9571\n",
      "Epoch 100/100\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.2879 - accuracy: 0.9512\n",
      "916.2046539\n"
     ]
    }
   ],
   "source": [
    "# The 'joke' database on GPU 154.4100482 seconds for 100 epochs.\n",
    "# The 'joke' database on CPU 439.003922 seconds for 100 epochs.\n",
    "\n",
    "start = timer()\n",
    "model.fit(X, y, epochs=100, verbose=1, batch_size=256)\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0d5ea",
   "metadata": {},
   "source": [
    "Make predictions using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e730a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed, n_words):\n",
    "    \n",
    "    text = seed\n",
    "    \n",
    "    # generate n_words\n",
    "    for _ in range(n_words):\n",
    "        \n",
    "        # encode text as integers\n",
    "        encoded_words = word_tokeniser.texts_to_sequences([text])[0]\n",
    "        \n",
    "        # pad sequences\n",
    "        padded_words = pad_sequences([encoded_words], maxlen=MAX_SEQ_LENGTH, padding='pre')\n",
    "        \n",
    "        # predict next word\n",
    "        prediction = model.predict_classes(padded_words, verbose=0)\n",
    "        \n",
    "        print(sorted(model.predict(padded_words)[0], reverse=True)[0:10])\n",
    "        \n",
    "        # convert predicted index to its word\n",
    "        next_word = \"\"\n",
    "        for word, i in word_tokeniser.word_index.items():\n",
    "            if i == prediction:\n",
    "                next_word = word\n",
    "                break\n",
    "        \n",
    "        # append predicted word to text\n",
    "        text += \" \" + next_word\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae905114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8252202, 0.053276382, 0.033418547, 0.025384123, 0.008905347, 0.005334601, 0.005212782, 0.0051278095, 0.0042188214, 0.0020693163]\n",
      "[0.19596015, 0.15580438, 0.10413958, 0.06441732, 0.06259328, 0.056783333, 0.05149593, 0.04895057, 0.033568017, 0.032676782]\n",
      "[0.5658212, 0.16607705, 0.07701397, 0.05759702, 0.028069556, 0.024884501, 0.022034148, 0.0133826, 0.0051111397, 0.003906514]\n",
      "[0.37619478, 0.08830888, 0.0490248, 0.043470196, 0.036576647, 0.022136692, 0.02110659, 0.019345714, 0.018209634, 0.017918926]\n",
      "[0.5818137, 0.07354214, 0.072661094, 0.059272252, 0.05616008, 0.02200624, 0.017967703, 0.011394227, 0.008659827, 0.0074720676]\n",
      "[0.55544144, 0.08749633, 0.07125034, 0.034787007, 0.028926538, 0.023420054, 0.0214832, 0.02142158, 0.020127857, 0.014584534]\n",
      "[0.24105245, 0.2001268, 0.14660127, 0.10593711, 0.06477123, 0.054210823, 0.034870327, 0.014184357, 0.012095779, 0.01190589]\n",
      "[0.24916677, 0.21834788, 0.21534108, 0.17182712, 0.06493011, 0.024553362, 0.014805362, 0.009584355, 0.007817026, 0.004944333]\n",
      "[0.4578023, 0.12992002, 0.09035393, 0.06968664, 0.056735963, 0.050677657, 0.031359255, 0.020889716, 0.013967791, 0.013635142]\n",
      "[0.3131674, 0.13430719, 0.097669095, 0.048210748, 0.04069567, 0.039044343, 0.030154906, 0.02835548, 0.026466891, 0.021840943]\n",
      "[0.27483383, 0.27429208, 0.18411744, 0.08988482, 0.040554307, 0.02649101, 0.024506176, 0.02175024, 0.011349303, 0.009059273]\n",
      "[0.53855944, 0.1762129, 0.17047481, 0.032530073, 0.02966701, 0.017569413, 0.0075451564, 0.0072185653, 0.0021343045, 0.0018000657]\n",
      "[0.18934505, 0.17199016, 0.1339149, 0.08597938, 0.052225772, 0.040314402, 0.030816834, 0.029040366, 0.025990278, 0.025831034]\n",
      "[0.19854693, 0.17815393, 0.113937624, 0.078981176, 0.070027456, 0.06895278, 0.05821692, 0.027773075, 0.026569104, 0.024994511]\n",
      "[0.74320775, 0.20779806, 0.012127438, 0.007990186, 0.007975303, 0.0028877635, 0.0025589387, 0.002281028, 0.0018716295, 0.0014818959]\n",
      "[0.1732889, 0.15250313, 0.122886285, 0.109403394, 0.10413665, 0.05160612, 0.03221207, 0.031914793, 0.030068865, 0.029769327]\n",
      "[0.73548037, 0.16167963, 0.06316937, 0.014364176, 0.008122277, 0.0027064472, 0.0027006823, 0.002130064, 0.0017869852, 0.0008927648]\n",
      "[0.4636687, 0.073170416, 0.048110835, 0.038648114, 0.03522699, 0.031696286, 0.028372617, 0.02728393, 0.024211217, 0.023615442]\n",
      "[0.31462428, 0.26465032, 0.07577515, 0.07555593, 0.035399634, 0.026790041, 0.024884485, 0.015596359, 0.012572645, 0.01121037]\n",
      "[0.2845076, 0.12208603, 0.09004876, 0.072237, 0.048489112, 0.045686744, 0.043341145, 0.041546278, 0.03235951, 0.028020507]\n",
      "Your mom is a smile but no part i shall break it 창 창 you may you contrive he lucas창 s table and herself\n"
     ]
    }
   ],
   "source": [
    "num_words = 20\n",
    "\n",
    "sentence = \"Your mom is a\"\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, sentence, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d468fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
